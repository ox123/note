MapReduce-day01

能听到声音的扣1
不能听到声音的扣2

一、介绍
    1、概述：
    MapReduce 是一个分布式运算程序的编程框架
    MapReduce 是一个分布式运算程序的编程模型

    半成品
    
    在一个完整的分布式的计算任务的代码编写过程中，对于程序员来说，除了业务代码之外其他所有的代码都不写是最好的。
    
    多线程，run方法
    
    随机函数
    
    通用的代码进行了封装，然后业务代码提供编写规范。
    
    2、作用：
    MapReduce的核心功能就是将用户编写的业务逻辑代码和自带的默认的组件整合成一个完整的分布式的运算程序，并发的运行在hadoop集群上面。
    
    3、普通程序升级到分布式程序相关问题
        1、数据重新传输回来
        2、在一个节点上面消耗的时间太长了
    
    4、分散计算的问题：
        （1）任务的分配
        （2）监控
        （3）容错
        （4）中间结果的汇总
    
    5、任务的阶段：
    （1）任务的并发
    （2）结果的汇总
    
    两个阶段的任务调度也是问题
    
    分布式运算程序编程框架，那就意味着要给绝大多数的业务场景通用的步骤给封装起来。
    
    6、总结
        1、数据存储的问题
            HDFS解决
        2、运算逻辑分为两个节点：map（并发就算）   reduce（结果汇总）
        3、两个阶段的计算如何启动、协调
        4、运算程序到底怎么执行的，到底是数据找计算还是计算找数据
        5、两个阶段的任务的分配
    
    MRAppMaster ： MapReduce Application Master


​    
    休息10min  21:40接着上课

二、MapReduce的编程流程图

三、WordCount
    在集群上面运行的命令：
    hadoop jar mr0722-1-1.0-SNAPSHOT.jar com.naixue.JobMain

    听懂了这个整个流程，没有问题，你在评论区扣1
    没听懂你扣2 

四、整体的架构分析
    这个在架构图中

五、分区组件
    在MapReduce当中，我们通过指定分区，会将同一个分区的数据发送到同一个Reducer当中进行处理

    其实就是相同类型的数据，有共性的数据，送到一起去处理
    
    需求：
        统计单词的案例，我要给结果进行进一步的划分，怎么划分？
        给单词的长度大于等于5的放到一个结果文件里面，给单词长度小于5的放到一个文件里面
    
    步骤：
        1、定义Mapper逻辑
        2、定义Reducer逻辑
        3、自定义的分区Partitioner
            这个案例主要的逻辑在这个里面
        4、Main入口

六、运行模式
    1、本地运行
        方便我们调试
    2、集群运行模式
        实际生产环境中使用

七、作业：
    帮分区的案例自己写一遍，写完之后，在下次上课之前交给班班或者助教。
    7月26号中午12点之前上交。

```java

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

import java.io.IOException;

public class JobMain {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

        Configuration configuration = new Configuration();
        //一、初始化一个job
        Job job = Job.getInstance(configuration, "sort");

        //二、设置job的相关信息  8个小步骤
        //1、设置输入的路径，让程序找到源文件
        job.setInputFormatClass(TextInputFormat.class);
        TextInputFormat.addInputPath(job,new Path("D://input/test3.txt"));

        //2、设置Mapper类，并设置k2 v2
        job.setMapperClass(SortMapper.class);
        job.setMapOutputKeyClass(MySortBean.class);
        job.setMapOutputValueClass(NullWritable.class);

        //3 4 5 6

        //7、设置Reducer类，并设置k3 v3
        job.setReducerClass(SortReducer.class);
        job.setOutputKeyClass(MySortBean.class);
        job.setOutputValueClass(NullWritable.class);

        //8、设置输出的路径
        job.setOutputFormatClass(TextOutputFormat.class);
        TextOutputFormat.setOutputPath(job,new Path("D://wordoutsort"));

        //三、等待完成
        boolean b = job.waitForCompletion(true);
        System.out.println(b);
        System.exit(b ? 0 : 1);
    }
}
```

Mysortbean

```java
import org.apache.hadoop.io.WritableComparable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

public class MySortBean implements WritableComparable<MySortBean> {

    private String word;
    private int num;

    public String getWord() {
        return word;
    }
    public void setWord(String word) {
        this.word = word;
    }
    public int getNum() {
        return num;
    }
    public void setNum(int num) {
        this.num = num;
    }
    @Override
    public String toString() {
        return "MySortBean{" +
                "word='" + word + '\'' +
                ", num=" + num +
                '}';
    }

    /**
     * 比较器，按照我们自己指定的规则进行排序
     * 排序规则：
     *      要求第一列按照字典顺序进行排列，第一列相同的时候, 第二列按照升序进行排列。
     * @param o
     * @return
     */
    @Override
    public int compareTo(MySortBean o) {
        //1、先对第一列进行排序 word
        int result = this.word.compareTo(o.word);

        //2、第一列相同的时候，num排序
        if (result == 0){
            return this.num - o.num;
        }
        return result;
    }

    //实现序列化
    @Override
    public void write(DataOutput out) throws IOException {
        out.writeUTF(word);
        out.writeInt(num);
    }

    //实现反序列化
    @Override
    public void readFields(DataInput in) throws IOException {
        this.word = in.readUTF();
        this.num = in.readInt();
    }
}

```

SortMapper

```java
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * @author LIAO
 * @create 2020-07-26 20:26
 * Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>
 *     KEYIN:文本偏移量
 *     VALUEIN：一行的文本
 *     KEYOUT：MySortBean
 *     VALUEOUT：NullWritable
 */
public class SortMapper extends Mapper<LongWritable,Text,MySortBean,NullWritable> {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        //1、拆分，将一行文本进行拆分
        String[] split = value.toString().split(" ");

        //2、将相应的值给写入到MySortBean对象中
        MySortBean mySortBean = new MySortBean();
        mySortBean.setWord(split[0]);
        mySortBean.setNum(Integer.parseInt(split[1]));

        //3、写入到上下文
        context.write(mySortBean,NullWritable.get());
    }
}

```

- SortReducer

```java
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

/**
 * 排序的Reducer
 * Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT>
 *     KEYIN:MySortBean map阶段的输出的key
 *     VALUEIN：NullWritable map阶段的输出的value
 *     KEYOUT：MySortBean
 *     VALUEOUT：NullWritable
 */
public class SortReducer extends Reducer<MySortBean,NullWritable,MySortBean,NullWritable>{
    @Override
    protected void reduce(MySortBean key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {
        //1、将k2 v2 转化为k3 v3 进行输出
        context.write(key,NullWritable.get());
    }
}
```

1. 什么是Combiner

   Combiner 是 MapReduce 程序中 Mapper 和 Reducer 之外的一种组件，它的作用是在 maptask之后给 maptask 的结果进行局部汇总，以减轻 reducetask 的计算负载，减少网络传输

1. 如何使用Combiner

Combiner 和 Reducer 一样，编写一个类，然后继承 Reducer，reduce 方法中写具体的
Combiner 逻辑，然后在 job 中设置 Combiner 组件：job.setCombinerClass(MyCombiner.class)  



