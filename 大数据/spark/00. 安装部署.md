

今晚内容大纲：
（1）Spark课程概述
（2）Spark简介
MapReduce的缺点和不足
Hadoop3.0-->对MR做了很大优化
Spark特点:快,易用,通用性,兼容性
什么是Spark

（3）Spark体系架构
官网图
http://spark.apache.org/docs/latest/cluster-overview.html

hive on spark 
首先部署spark on  yarn 再部署hive on spark

（4）搭建Spark环境

（5）Spark工具的使用submit
（6）Spark工具使用shell
（7）开发java版本的Spark程序

（8）开发scala版本的Spark程序

（9）Spark提交任务的流程 类似于yarn流程

（10）分析WordCount程序处理的过程

作业:
1.安装部署自己的spark环境
2.画一遍体系架构图
3.写一个java版本的spark的WordCount,提交到集群运行一下.

（1）
Spark课程概述
前提:scala 也支持java

三部分内容:
1.SparkCore.内核,是spark里面最重要的内容,相当于MapReduce

Spark core和mapreduce都是进行离线计算(hdfs,Hbase)
sparkcore核心:RDD 弹性分布式数据集,由分区组成

2.Spark SQL 相当于Hive
支持SQL和DSL语句-->Spark任务(RDD)-->sparkcore运行

3.SparkStreaming 相当于storm
本质:把连续的数据转换成不连续的数据.DStream 离散流-->本质就是RDD

Spark core部分三天大纲:
一.什么是Spark
1.为什么要学习Spark,谈论MapReduce的缺点和不足
spark的特点

二.Spark的体系架构与部署
1.体系架构:主从架构(单点故障)
2.安装部署
(1)伪分布:一个主节点,一个从节点
(2)全分布:至少三台

3.Spark的HA
(1)基于文件目录:开发测试
(2)基于zookeeper:生产环境

三.执行Spark Demo程序
1.执行spark的任务的工具
(1)spark-submit:相当于hadoop jar命令-->提交MapReduce任务(jar文件)
examples
/root/training/spark-2.1.0-bin-hadoop2.7/examples

SparkPi.scala  蒙特卡罗求Pi
org.apache.spark.examples.SparkPi

./bin/spark-submit --master spark://bigdata111:7077 --class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.11-2.1.0.jar 200

(2)sparkshell:类似于ScalaREPL命令行,Spark交互式命令行
a.本地模式  bin/spark-shell 
Spark context available as 'sc' (master = local[*], app id = local-1600293405212).
.setMaster("local")

b.集群模式
bin/spark-shell --master spark://bigdata111:7077 
Spark context available as 'sc' (master = spark://bigdata111:7077, app id = app-20200917055957-0001).

spark-shell

hdfs://bigdata11:9000/input/data.txt

root/temp/data.txt
 sc.textFile("hdfs://bigdata111:9000/input/data.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://bigdata111:9000/output/spark/wc111")

只产生一个分区.
sc.textFile("hdfs://bigdata111:9000/input/data.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).repartition(1).saveAsTextFile("hdfs://bigdata111:9000/output/spark/wc112")

lazy 懒值

rdd2.map(word=>(word,1))

reduceByKey(_+_)---->reduceByKey((a,b)=>a+b)

数组:

Array(
(Tom,1),
(Tom,2),
(Mary,3),
(Tom,4)
)
.reduceByKey(_+_)

先分组:reduceByKey -->(Tom,(1,2,4)) (Mary,(3))
每组求和:_+_=>累加  1+2=3 3+4=7   
单步执行WordCount总结:
(1)transformation 延迟  action 立即执行
(2)RDD之间有依赖关系(宽依赖,窄依赖)

(3)程序

a.Java版本

./bin/spark-submit --master spark://bigdata111:7077 --class nx.MyJavaWordCount ~/tools/temp/MyJavaWordCount.jar hdfs://bigdata111:9000/input/data.txt

b.Scala版本

四.Spark执行原理分析
1.分析WordCount程序处理的过程
2.Spark提交任务的流程:类似于yarn调度任务的过程

五.Spark算子(函数)
1.重要:RDD(类)
Resident 弹性
distributed dataset (分布式数据集)

5个特性

* - A list of partitions
    RDD是一组分区

* - A function for computing each split
    包含函数/算子,用于处理RDD数据

* - A list of dependencies on other RDDs
    依赖关系(宽依赖,窄依赖)

* - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)

* - Optionally, a list of preferred locations to compute each split on (e.g. block locations for
* an HDFS file)

创建RDD
1.
Array(1,2,3,4,5,6,7,8,9,10)
val rdd1=sc.parallelize(Array(1,2,3,4,5,6,7,8,9,10),3)
2.
读取外部数据源
hdfs,本地
val rdd2 =sc.textFile("hdfs://bigdata111:9000/input/data.text")
val rdd3 =sc.textFile("root/tools/temp/data.txt")

spark sql   表 DataFrame -->RDD  json parquet ...
spark streaming 流式计算 Dstream  输入流  -->RDD

2.RDD的算子(函数或者方法)
(1)transformation--->不会触发计算
map(word =>(word,1))
flatMap==>flatten(压平)+map

reduceBykey会有一个本地操作,相当于有一个combiner

(2)action--->会触发计算

22:05接着上课 
transformation
val rdd1=sc.parallelize(Array(5,6,7,8,1,2,3,100,30),3)

val rdd2 = rdd1.map(_*2).sortBy(x=>x,true)

val rdd3 =rdd2.filter(_>50)

val rdd4 =sc.parallelize(Array("a b c","d e f","x y z"))

val rdd5 =rdd4.flatMap(_.split(" "))

union intersect

val rdd6=sc.parallelize(List(5,6,7,8,1,2,3,100,30))
val rdd7=sc.parallelize(List(1,2,3,4,5,6,7,8,9,10))
val rdd8=rdd6.union(rdd7)
val rdd9=rdd6.intersection(rdd7)

(k,v)
val rdd10 =sc.parallelize(List(("Tom",1000),("Jerry",3000),("Mary",2000)))
val rdd11 =sc.parallelize(List(("Jerry",1000),("Tom",3000),("Mike",2000)))

val rdd12 =rdd10.union(rdd11)
val rdd13 =rdd12.groupBykey

 Array(
 (Tom,CompactBuffer(1000, 3000)), 
 (Jerry,CompactBuffer(3000, 1000)), 
 (Mike,CompactBuffer(2000)), 
 (Mary,CompactBuffer(2000)))

action
rdd13.count
rdd13.take(2)

3.RDD特性

RDD的缓存机制(默认缓存在在内存)
persist  cache

  /**

* Persist this RDD with the default storage level (`MEMORY_ONLY`).
  */
  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)
  
  /**

* Persist this RDD with the default storage level (`MEMORY_ONLY`).
  */
  def cache(): this.type = persist()
  
  hdfs dfs -mkdir /oracle
  hdfs dfs -put ~/tools/temp/sales /oracle

  val rdd1 =sc.textFile("hdfs://bigdata111:9000/oracle/sales")
  rdd1.count
  rdd1.cache 
  rdd1.count
  rdd1.count

RDD的容错机制  checkpoint

hdfs 检查点 secondaryNameNode

每一步产生RDD -->RDD越多-->依赖关系越长--->任务的血统越长lineage-->出错概率越高
基于内存

两种方式检查点:
1.本地目录
2.基于hdfs

sc.setCheckpointDir("hdfs://bigdata111:9000/checkpoint0920")
val rdd1 =sc.textFile("hdfs://bigdata111:9000/oracle/sales")
rdd1.checkpoint
rdd1.count

缺点: IO  

RDD的依赖关系

RDD高级算子

六.Spark编程案例
